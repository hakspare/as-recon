#!/usr/bin/env python3
import requests, urllib3, sys, concurrent.futures, re, time, argparse, socket, hashlib, string
from random import choices

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

C, G, Y, R, M, W, B = '\033[96m', '\033[92m', '\033[93m', '\033[91m', '\033[95m', '\033[0m', '\033[1m'

LOGO = f"""{C}{B}
   ‚ñÑ‚ñÑ‚ñÑ¬∑ .‚ñÑ‚ñÑ ¬∑      ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ . ‚ñÑ‚ñÑ¬∑       ‚ñê ‚ñÑ 
  ‚ñê‚ñà ‚ñÄ‚ñà ‚ñê‚ñà ‚ñÄ. ‚ñ™     ‚Ä¢‚ñà‚ñà  ‚ñÄ‚ñÑ.‚ñÄ¬∑‚ñê‚ñà ‚ñÑ¬∑‚ñ™     ‚Ä¢‚ñà‚ñå‚ñê‚ñà
  ‚ñÑ‚ñà‚ñÄ‚ñÄ‚ñà ‚ñÑ‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñÑ ‚ñÑ‚ñà‚ñÄ‚ñÑ  ‚ñê‚ñà.‚ñ™‚ñê‚ñÄ‚ñÄ‚ñ™‚ñÑ‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ‚ñà‚ñÄ‚ñÑ  ‚ñê‚ñà‚ñê‚ñê‚ñå
  ‚ñê‚ñà ‚ñ™‚ñê‚ñå‚ñê‚ñà‚ñÑ‚ñ™‚ñê‚ñà‚ñê‚ñà‚ñå.‚ñê‚ñå ‚ñê‚ñà‚ñå¬∑‚ñê‚ñà‚ñÑ‚ñÑ‚ñå‚ñê‚ñà ‚ñ™‚ñê‚ñà‚ñê‚ñà‚ñå.‚ñê‚ñå‚ñà‚ñà‚ñê‚ñà‚ñå
   ‚ñÄ  ‚ñÄ  ‚ñÄ‚ñÄ‚ñÄ‚ñÄ  ‚ñÄ‚ñà‚ñÑ‚ñÄ‚ñ™ ‚ñÄ‚ñÄ‚ñÄ  ‚ñÄ‚ñÄ‚ñÄ  ‚ñÄ  ‚ñÄ ‚ñÄ‚ñà‚ñÑ‚ñÄ‚ñ™‚ñÄ‚ñÄ ‚ñà‚ñ™
{Y}        >> AS-RECON v10.2: Overlord Engine <<{W}
{G}      Developed by Ajijul Islam Shohan (@hakspare){W}
"""

def clean_subdomain(sub, domain):
    """
    üëâ ‡¶è‡¶á ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Filtering Perfect ‡¶ï‡¶∞‡¶¨‡ßá‡•§
    ‡¶è‡¶ü‡¶ø Dirty Data, Regex Over-match ‡¶è‡¶¨‡¶Ç Wordlist Leak ‡¶´‡¶ø‡¶ï‡ßç‡¶∏ ‡¶ï‡¶∞‡¶¨‡ßá‡•§
    """
    sub = sub.lower().strip()
    # ‡ßß. ‡¶∏‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶ü ‡¶™‡¶æ‡¶∞‡ßç‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶ì‡ßü‡¶æ‡¶á‡¶≤‡ßç‡¶°‡¶ï‡¶æ‡¶∞‡ßç‡¶° ‡¶´‡¶ø‡¶ï‡ßç‡¶∏
    if sub.startswith("*."): sub = sub[2:]
    if sub.startswith("."): sub = sub[1:]
    
    # ‡ß®. Regex Over-match ‡¶´‡¶ø‡¶ï‡ßç‡¶∏ (‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶∞‡¶æ‡¶ñ‡¶æ)
    # ‡¶è‡¶ü‡¶ø ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶®‡ßá‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶Ü‡¶¨‡¶∞‡ßç‡¶ú‡¶®‡¶æ (‡¶Ø‡ßá‡¶Æ‡¶® ", /, ?, >) ‡¶ï‡ßá‡¶ü‡ßá ‡¶´‡ßá‡¶≤‡ßá
    match = re.search(r'([a-z0-9-.]+\.' + re.escape(domain) + r')', sub)
    if match:
        sub = match.group(1)
    
    # ‡ß©. Dirty Data Filtering (‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶æ‡¶¶)
    if not all(c in (string.ascii_lowercase + string.digits + ".-") for c in sub):
        return None
    
    # ‡ß™. ‡¶°‡¶æ‡¶¨‡¶≤ ‡¶°‡¶ü ‡¶¨‡¶æ ‡¶≠‡ßÅ‡¶≤ ‡¶´‡¶∞‡¶Æ‡ßá‡¶ü ‡¶´‡¶ø‡¶ï‡ßç‡¶∏
    if ".." in sub or sub == domain:
        return None
        
    return sub

def fetch_source(url, domain):
    try:
        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10, verify=False)
        if r.status_code == 200:
            # ‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞‡¶´‡ßÅ‡¶≤ Regex ‡¶Ø‡¶æ ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶®‡ßá‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ö‡ßá‡¶®‡ßá
            pattern = r'(?:[a-zA-Z0-9-]+\.)+' + re.escape(domain)
            raw_subs = re.findall(pattern, r.text)
            
            cleaned = []
            for s in raw_subs:
                c = clean_subdomain(s, domain)
                if c: cleaned.append(c)
            return cleaned
    except: pass
    return []

# ... [‡¶¨‡¶æ‡¶ï‡¶ø Intelligence ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶è‡¶¨‡¶Ç check_live ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã‡¶á ‡¶•‡¶æ‡¶ï‡¶¨‡ßá] ...

def main():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=LOGO, add_help=False)
    target_grp = parser.add_argument_group(f'{Y}TARGET OPTIONS{W}')
    target_grp.add_argument("-d", "--domain", metavar="DOMAIN", required=True, help="Target domain")
    
    mode_grp = parser.add_argument_group(f'{Y}SCAN MODES{W}')
    mode_grp.add_argument("--live", action="store_true", help="Check live hosts")
    
    perf_grp = parser.add_argument_group(f'{Y}PERFORMANCE{W}')
    perf_grp.add_argument("-t", "--threads", type=int, default=60, help="Threads")
    
    out_grp = parser.add_argument_group(f'{Y}OUTPUT{W}')
    out_grp.add_argument("-o", "--output", help="Save result")
    
    sys_grp = parser.add_argument_group(f'{Y}SYSTEM{W}')
    sys_grp.add_argument("-h", "--help", action="help", help="Show help")

    if len(sys.argv) == 1:
        print(LOGO); parser.print_help(); sys.exit()
    args = parser.parse_args()

    # --- Scanning Logic with Improved Filtering ---
    start_time = time.time()
    target = args.domain
    
    # Sources list
    sources = [
        f"https://crt.sh/?q=%25.{target}",
        f"https://api.subdomain.center/api/index.php?domain={target}",
        f"https://otx.alienvault.com/api/v1/indicators/domain/{target}/passive_dns",
        f"https://api.hackertarget.com/hostsearch/?q={target}",
        f"https://jldc.me/anubis/subdomains/{target}"
    ]

    print(f"{C}[*] Hunting Subdomains for: {target}{W}")
    
    raw_results = set()
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        futures = {executor.submit(fetch_source, url, target): url for url in sources}
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: raw_results.update(res)

    # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡ßç‡¶ü‡¶ø‡¶Ç
    final_list = sorted(list(raw_results))

    print(f"{G}[+]{W} Unique Cleaned Targets: {B}{len(final_list)}{W}\n")
    
    # ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ö‡ßá‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø ‡¶ï‡ßã‡¶° (‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã)
    # ... [Summary and Output Logic] ...
